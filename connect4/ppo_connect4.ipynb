{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses Stable-Baselines3 to train agents in the Connect Four environment using invalid action masking.\n",
    "\n",
    "For information about invalid action masking in PettingZoo, see https://pettingzoo.farama.org/api/aec/#action-masking\n",
    "For more information about invalid action masking in SB3, see https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html\n",
    "\n",
    "Author: Elliot (https://github.com/elliottower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pettingzoo.utils\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\"\"\"\n",
    "        super().step(action)\n",
    "        return super().last()\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env):\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.action_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action_mask(env_fn, steps=10_000, seed=0, **env_kwargs):\n",
    "    \"\"\"Train a single model to play as each agent in a zero-sum game environment using invalid action masking.\"\"\"\n",
    "    env = env_fn.env(**env_kwargs)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    # Custom wrapper to convert PettingZoo envs to work with SB3 action masking\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "\n",
    "    env.reset(seed=seed)  # Must call reset() in order to re-define the spaces\n",
    "\n",
    "    env = ActionMasker(env, mask_fn)  # Wrap to enable masking (SB3 function)\n",
    "    # MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "    # with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "    # retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "    # a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import os\n",
    "\n",
    "current_folder = globals()[\"_dh\"][0]\n",
    "\n",
    "\n",
    "def eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[1]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(\n",
    "                os.path.join(current_folder, \"models\", f\"{env.metadata['name']}*.zip\")\n",
    "            ),\n",
    "            key=os.path.getctime,\n",
    "        )\n",
    "        print(f\"Using policy {latest_policy}\")\n",
    "        model = MaskablePPO.load(latest_policy)\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "            sleep(0.3)\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    scores[winner] += env.rewards[\n",
    "                        winner\n",
    "                    ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[1]] / sum(scores.values())\n",
    "    print(\"Rewards by round: \", round_rewards)\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return round_rewards, total_rewards, winrate, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jdmas\\OneDrive\\Documents\\GitHub\\darwin\\connect4\\ppo_connect4.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m env_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Evaluation/training hyperparameter notes:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# 10k steps: Winrate:  0.76, loss order of 1e-03\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 20k steps: Winrate:  0.86, loss order of 1e-04\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 40k steps: Winrate:  0.86, loss order of 7e-06\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Train a model against itself (takes ~20 seconds on a laptop CPU)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_action_mask(env_fn, steps\u001b[39m=\u001b[39;49m\u001b[39m20_480\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49menv_kwargs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Evaluate 100 games against a random agent (winrate should be ~80%)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m eval_action_mask(env_fn, num_games\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, render_mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_kwargs)\n",
      "\u001b[1;32mc:\\Users\\jdmas\\OneDrive\\Documents\\GitHub\\darwin\\connect4\\ppo_connect4.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_action_mask\u001b[39m(env_fn, steps\u001b[39m=\u001b[39m\u001b[39m10_000\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_kwargs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Train a single model to play as each agent in a zero-sum game environment using invalid action masking.\"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env \u001b[39m=\u001b[39m env_fn\u001b[39m.\u001b[39;49menv(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_kwargs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting training on \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(env\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jdmas/OneDrive/Documents/GitHub/darwin/connect4/ppo_connect4.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Custom wrapper to convert PettingZoo envs to work with SB3 action masking\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_fn = 'connect_four_v3'\n",
    "\n",
    "    env_kwargs = {}\n",
    "\n",
    "    # Evaluation/training hyperparameter notes:\n",
    "    # 10k steps: Winrate:  0.76, loss order of 1e-03\n",
    "    # 20k steps: Winrate:  0.86, loss order of 1e-04\n",
    "    # 40k steps: Winrate:  0.86, loss order of 7e-06\n",
    "\n",
    "    # Train a model against itself (takes ~20 seconds on a laptop CPU)\n",
    "    train_action_mask(env_fn, steps=20_480, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 100 games against a random agent (winrate should be ~80%)\n",
    "    eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs)\n",
    "\n",
    "    # Watch two games vs a random agent\n",
    "    eval_action_mask(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
